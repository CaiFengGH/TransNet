{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### TransNet: Translation-Based Network Representation Learning for Social Relation Extraction\n",
    "\n",
    "This is a transNet implementation example using TensorFlow library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from input_data import read_data_sets\n",
    "\n",
    "aminer = read_data_sets()\n",
    "entity_total = aminer.entity_total\n",
    "tag_total = aminer.tag_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "warm_up_epochs = 40\n",
    "epochs = 200\n",
    "batch_size = 200\n",
    "eval_batch_size = 2000\n",
    "display_step = 5\n",
    "\n",
    "gamma = 1\n",
    "alpha = 0.5\n",
    "l2_lambda = 0.001\n",
    "beta = 50.0\n",
    "keep_prob = 0.5\n",
    "rep_size = 64\n",
    "\n",
    "hits_k = [1,2,3,4,5,6,7,8,9,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input\n",
    "pos_h = tf.placeholder(tf.int32, [None])\n",
    "pos_t = tf.placeholder(tf.int32, [None])\n",
    "pos_r = tf.placeholder(tf.float32, [None, tag_total])\n",
    "pos_br = tf.placeholder(tf.float32, [None, tag_total])\n",
    "\n",
    "neg_h = tf.placeholder(tf.int32, [None])\n",
    "neg_t = tf.placeholder(tf.int32, [None])\n",
    "neg_r = tf.placeholder(tf.float32, [None, tag_total])\n",
    "neg_br = tf.placeholder(tf.float32, [None, tag_total])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#embedding\n",
    "node_lookup = {\n",
    "    'int_embeddings': tf.Variable(tf.random_normal([entity_total, rep_size])),\n",
    "    'adv_embeddings': tf.Variable(tf.random_normal([entity_total, rep_size])),\n",
    "}\n",
    "\n",
    "def lookup(pos_head, pos_tail, neg_head, neg_tail, lookup):\n",
    "    pos_head_e = tf.nn.l2_normalize(\n",
    "        tf.nn.embedding_lookup(lookup['int_embeddings'], pos_head), 1)\n",
    "    pos_tail_e = tf.nn.l2_normalize(\n",
    "        tf.nn.embedding_lookup(lookup['adv_embeddings'], pos_tail), 1)\n",
    "    neg_head_e = tf.nn.l2_normalize(\n",
    "        tf.nn.embedding_lookup(lookup['int_embeddings'], neg_head), 1)\n",
    "    neg_tail_e = tf.nn.l2_normalize(\n",
    "        tf.nn.embedding_lookup(lookup['adv_embeddings'], neg_tail), 1)\n",
    "    return pos_head_e, pos_tail_e, neg_head_e, neg_tail_e\n",
    "\n",
    "pos_h_e, pos_t_e, neg_h_e, neg_t_e = lookup(pos_h, pos_t,\n",
    "                                           neg_h, neg_t, node_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#autoencoder\n",
    "relation_weights = {\n",
    "    'encoder_w': tf.Variable(tf.random_normal([tag_total, rep_size])),\n",
    "    'decoder_w': tf.Variable(tf.random_normal([rep_size, tag_total])),\n",
    "}\n",
    "relation_biases = {\n",
    "    'encoder_b': tf.Variable(tf.random_normal([rep_size])),\n",
    "    'decoder_b': tf.Variable(tf.random_normal([tag_total])),\n",
    "}\n",
    "\n",
    "def autoencoder(W,B,x):\n",
    "    rep = tf.nn.dropout(\n",
    "        tf.nn.tanh(tf.matmul(x, W['encoder_w'])+B['encoder_b']), keep_prob)\n",
    "    decode_x = tf.nn.sigmoid(\n",
    "        tf.matmul(rep, W['decoder_w'])+B['decoder_b'])\n",
    "    return rep, decode_x\n",
    "\n",
    "pos_r_rep, pos_r_dec = autoencoder(relation_weights, relation_biases, pos_r)\n",
    "neg_r_rep, neg_r_dec = autoencoder(relation_weights, relation_biases, neg_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loss\n",
    "relation_ae_l2_loss = tf.nn.l2_loss(relation_weights['encoder_w'])+\\\n",
    "                        tf.nn.l2_loss(relation_weights['decoder_w'])+\\\n",
    "                        tf.nn.l2_loss(relation_biases['encoder_b'])+\\\n",
    "                        tf.nn.l2_loss(relation_biases['decoder_b'])\n",
    "relation_loss = tf.reduce_sum(tf.abs(tf.multiply(pos_r_dec-pos_r, pos_br)))+\\\n",
    "                tf.reduce_sum(tf.abs(tf.multiply(neg_r_dec-neg_r, neg_br)))\n",
    "relation_pos_r_loss = tf.reduce_sum(tf.abs(tf.multiply(pos_r_dec-pos_r, pos_br))) +\\\n",
    "                        l2_lambda*relation_ae_l2_loss\n",
    "\n",
    "pos = tf.reduce_sum(tf.abs(pos_h_e + pos_r_rep - pos_t_e), 1, keep_dims = True)\n",
    "neg = tf.reduce_sum(tf.abs(neg_h_e + neg_r_rep - neg_t_e), 1, keep_dims = True)\n",
    "trans_loss = tf.reduce_sum(tf.maximum(pos - neg + gamma, 0))\n",
    "loss = trans_loss+alpha*relation_loss+l2_lambda*relation_ae_l2_loss\n",
    "\n",
    "relation_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(relation_pos_r_loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "relation_sum = tf.reduce_sum(pos_r)\n",
    "pos_r_minus = pos_t_e - pos_h_e\n",
    "pos_r_minus_dec = tf.nn.sigmoid(\n",
    "    tf.matmul(pos_r_minus, relation_weights['decoder_w'])+relation_biases['decoder_b'])\n",
    "hits = []\n",
    "for k in hits_k:\n",
    "    topk_indices = tf.nn.top_k(pos_r_minus_dec, k=k).indices\n",
    "    pred = tf.reduce_sum(tf.one_hot(topk_indices, tag_total), 1)\n",
    "    hits.append(tf.reduce_sum(tf.multiply(pred, pos_r)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting warm-up relation\n",
      "Warm-up relation epoch:  0 sum of loss 37099769.3789\n",
      "Warm-up relation epoch:  1 sum of loss 14118158.5811\n",
      "Warm-up relation epoch:  2 sum of loss 8842641.18115\n",
      "Warm-up relation epoch:  3 sum of loss 6534123.71191\n",
      "Warm-up relation epoch:  4 sum of loss 5151475.13013\n",
      "Warm-up relation epoch:  5 sum of loss 4277016.4248\n",
      "Warm-up relation epoch:  6 sum of loss 3668801.74219\n",
      "Warm-up relation epoch:  7 sum of loss 3286632.22778\n",
      "Warm-up relation epoch:  8 sum of loss 2984517.66162\n",
      "Warm-up relation epoch:  9 sum of loss 2792923.52893\n",
      "Warm-up relation epoch:  10 sum of loss 2634050.89917\n",
      "Warm-up relation epoch:  11 sum of loss 2515474.53491\n",
      "Warm-up relation epoch:  12 sum of loss 2434859.34949\n",
      "Warm-up relation epoch:  13 sum of loss 2323524.02661\n",
      "Warm-up relation epoch:  14 sum of loss 2236894.51965\n",
      "Warm-up relation epoch:  15 sum of loss 2147938.34827\n",
      "Warm-up relation epoch:  16 sum of loss 2084366.06323\n",
      "Warm-up relation epoch:  17 sum of loss 2045124.78101\n",
      "Warm-up relation epoch:  18 sum of loss 2003976.39807\n",
      "Warm-up relation epoch:  19 sum of loss 1955302.05634\n",
      "Warm-up relation epoch:  20 sum of loss 1913112.65314\n",
      "Warm-up relation epoch:  21 sum of loss 1868866.23395\n",
      "Warm-up relation epoch:  22 sum of loss 1844474.79895\n",
      "Warm-up relation epoch:  23 sum of loss 1813458.49292\n",
      "Warm-up relation epoch:  24 sum of loss 1787107.16376\n",
      "Warm-up relation epoch:  25 sum of loss 1723744.43811\n",
      "Warm-up relation epoch:  26 sum of loss 1701702.47742\n",
      "Warm-up relation epoch:  27 sum of loss 1686509.59583\n",
      "Warm-up relation epoch:  28 sum of loss 1657243.23901\n",
      "Warm-up relation epoch:  29 sum of loss 1643128.44037\n",
      "Warm-up relation epoch:  30 sum of loss 1631804.05591\n",
      "Warm-up relation epoch:  31 sum of loss 1622722.73615\n",
      "Warm-up relation epoch:  32 sum of loss 1604044.30341\n",
      "Warm-up relation epoch:  33 sum of loss 1605583.48505\n",
      "Warm-up relation epoch:  34 sum of loss 1555334.86151\n",
      "Warm-up relation epoch:  35 sum of loss 1549908.88245\n",
      "Warm-up relation epoch:  36 sum of loss 1531816.36414\n",
      "Warm-up relation epoch:  37 sum of loss 1515304.823\n",
      "Warm-up relation epoch:  38 sum of loss 1442369.54041\n",
      "Warm-up relation epoch:  39 sum of loss 1428704.71893\n",
      "Train TransNet epoch:  0 sum of loss 19010779.915\n",
      "Evaluating...\n",
      "Recall [0.0072928205939758401, 0.012686606530569184, 0.018449649206098012, 0.023592868069842274, 0.028815213377644141, 0.033364983910956374, 0.038086195073060082, 0.042359023052170701, 0.046803291660072799, 0.051168433823917288]\n",
      "Precision [0.027650000000000001, 0.024049999999999998, 0.023316666666666666, 0.0223625, 0.021850000000000001, 0.021083333333333332, 0.02062857142857143, 0.020074999999999999, 0.019716666666666667, 0.019400000000000001]\n",
      "Train TransNet epoch:  1 sum of loss 17811928.4795\n",
      "Evaluating...\n",
      "Recall [0.014110882523606056, 0.025280898876404494, 0.034802447644669512, 0.043545919713034766, 0.051550878303529037, 0.059265706599145436, 0.066650841377855152, 0.073482091048161627, 0.080089149126971571, 0.086564329799018833]\n",
      "Precision [0.053499999999999999, 0.047925000000000002, 0.043983333333333333, 0.041274999999999999, 0.03909, 0.037449999999999997, 0.0361, 0.034825000000000002, 0.033738888888888888, 0.032820000000000002]\n",
      "Train TransNet epoch:  2 sum of loss 16755054.9502\n",
      "Evaluating...\n",
      "Recall [0.02509627050693675, 0.043150287492746742, 0.058500817639921932, 0.072202880202563699, 0.084876298992456606, 0.096099066307960126, 0.10604262277786569, 0.11559054702748325, 0.1250197816110144, 0.13367093949464579]\n",
      "Precision [0.095149999999999998, 0.081799999999999998, 0.073933333333333337, 0.068437499999999998, 0.064360000000000001, 0.060725000000000001, 0.057435714285714284, 0.054781249999999997, 0.052666666666666667, 0.050680000000000003]\n",
      "Train TransNet epoch:  3 sum of loss 15895282.627\n",
      "Evaluating...\n",
      "Recall [0.039840164583003643, 0.06741573033707865, 0.090045893337553404, 0.10962968824181041, 0.12644405760405128, 0.14154402067837737, 0.15508783035290394, 0.16776124914279686, 0.17939283641926465, 0.19047053858732921]\n",
      "Precision [0.15104999999999999, 0.1278, 0.1138, 0.1039125, 0.095880000000000007, 0.089441666666666669, 0.084000000000000005, 0.079506250000000001, 0.075572222222222221, 0.072215000000000001]\n",
      "Train TransNet epoch:  4 sum of loss 15255635.5332\n",
      "Evaluating...\n",
      "Recall [0.057089729387561322, 0.094833043203038453, 0.12565279316347525, 0.15154032811098803, 0.1744474336656644, 0.19403122856992139, 0.21141267078124176, 0.22718520863005751, 0.24190272722477185, 0.25551247560267976]\n",
      "Precision [0.21645, 0.17977499999999999, 0.1588, 0.1436375, 0.13228000000000001, 0.12260833333333333, 0.11450714285714286, 0.10766874999999999, 0.10190555555555555, 0.096875000000000003]\n",
      "Train TransNet epoch:  5 sum of loss 14748211.2764\n",
      "Evaluating...\n",
      "Recall [0.075539378593659334, 0.12565279316347525, 0.16443793849237748, 0.19749960436777972, 0.22655219707759666, 0.25096270506936752, 0.27124545022946667, 0.29003798069314762, 0.30594239594872608, 0.32124017513319619]\n",
      "Precision [0.28639999999999999, 0.2382, 0.20781666666666668, 0.18720000000000001, 0.17179, 0.15858333333333333, 0.14691428571428572, 0.13745625, 0.12888333333333332, 0.121795]\n",
      "Train TransNet epoch:  6 sum of loss 14310792.8633\n",
      "Evaluating...\n",
      "Recall [0.094305533575987768, 0.15556258901724956, 0.20402753600253204, 0.24434245924988132, 0.27905259270981697, 0.30757767579258322, 0.33089360130822387, 0.35109722002426547, 0.36930948989819062, 0.38562272511473333]\n",
      "Precision [0.35754999999999998, 0.2949, 0.25785000000000002, 0.2316, 0.21160000000000001, 0.19435833333333333, 0.17922142857142856, 0.16639375000000001, 0.15557777777777779, 0.146205]\n",
      "Train TransNet epoch:  7 sum of loss 13877412.1562\n",
      "Evaluating...\n",
      "Recall [0.11234636282112148, 0.18479981009653426, 0.24217966977897346, 0.28841588858996675, 0.32819011446958907, 0.3609748377907897, 0.38658543018410085, 0.40926834414728069, 0.42940602415994095, 0.44751279210845596]\n",
      "Precision [0.42595, 0.350325, 0.30606666666666665, 0.27337499999999998, 0.24886, 0.2281, 0.20938571428571429, 0.19396250000000001, 0.18089444444444444, 0.16966999999999999]\n",
      "Train TransNet epoch:  8 sum of loss 13534085.2373\n",
      "Evaluating...\n",
      "Recall [0.12788152133776443, 0.21129398111515535, 0.27758875349475126, 0.32975945561006487, 0.37507253257371947, 0.41342248246030489, 0.44053647729071055, 0.46445903887745954, 0.48640343936276836, 0.50497177823495276]\n",
      "Precision [0.48485, 0.40055000000000002, 0.35081666666666667, 0.31256250000000002, 0.28441, 0.26124166666666665, 0.23860714285714285, 0.22011875, 0.20490555555555556, 0.19145499999999999]\n",
      "Train TransNet epoch:  9 sum of loss 13264467.6797\n",
      "Evaluating...\n",
      "Recall [0.14212428126813315, 0.23755077280160364, 0.31129661866329061, 0.37062826396581738, 0.42062298886954685, 0.4622434984438466, 0.4923379226670887, 0.51723637706388137, 0.53953684654744949, 0.55910745371103021]\n",
      "Precision [0.53885000000000005, 0.45032499999999998, 0.39341666666666669, 0.3513, 0.31895000000000001, 0.29209166666666669, 0.26666428571428569, 0.24513124999999999, 0.2272888888888889, 0.21198]\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "total_batch = int(aminer.train.num_examples / batch_size)\n",
    "test_total_batch = int(aminer.test.num_examples / batch_size)\n",
    "\n",
    "# initialize relation\n",
    "print \"Starting warm-up relation\"\n",
    "for epoch in range(warm_up_epochs):\n",
    "    # loop over all batches\n",
    "    sum_loss = 0.0\n",
    "    for i in range(total_batch):\n",
    "        vecs, bs = aminer.train.next_autoencoder_batch(batch_size, beta)\n",
    "        _, cur_loss = sess.run([relation_optimizer, relation_pos_r_loss],\n",
    "                              feed_dict={pos_r: vecs, pos_br: bs})\n",
    "        sum_loss += cur_loss\n",
    "    print 'Warm-up relation epoch: ', epoch, 'sum of loss', sum_loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    sum_loss = 0.0\n",
    "    for i in range(total_batch):\n",
    "        pos_h_batch, pos_t_batch, pos_r_batch, pos_b_batch,\\\n",
    "        neg_h_batch, neg_t_batch, neg_r_batch, neg_b_batch = aminer.train.next_batch(batch_size, beta)\n",
    "        _, cur_loss = sess.run([optimizer, loss],\n",
    "                               feed_dict={pos_h: pos_h_batch, pos_t: pos_t_batch,\n",
    "                                         pos_r: pos_r_batch, pos_br: pos_b_batch,\n",
    "                                         neg_h: neg_h_batch, neg_t: neg_t_batch,\n",
    "                                         neg_r: neg_r_batch, neg_br: neg_b_batch})\n",
    "        sum_loss += cur_loss\n",
    "    print 'Train TransNet epoch: ', epoch, 'sum of loss', sum_loss\n",
    "    if epoch % display_step == 0:\n",
    "        print 'Evaluating...'\n",
    "        hits_ = [0]*len(hits_k)\n",
    "        p_ = [0]*len(hits_k)\n",
    "        p_indice = [float(i+1) for i in range(len(hits))]\n",
    "        all_count = 0.0\n",
    "        for i in range(test_total_batch):\n",
    "            pos_h_batch, pos_t_batch, pos_r_batch = aminer.test.next_test_batch(eval_batch_size)\n",
    "            cur_hits, cur_sum = sess.run([hits, relation_sum],\n",
    "                                        feed_dict={pos_h: pos_h_batch,\n",
    "                                                  pos_t: pos_t_batch,\n",
    "                                                  pos_r: pos_r_batch})\n",
    "            hits_ = list(map(lambda x: x[0]+x[1], zip(hits_, cur_hits)))\n",
    "            p_value = [len(pos_r_batch)*indice for indice in p_indice]\n",
    "            p_ = list(map(lambda x: x[0]+x[1], zip(p_, p_value)))\n",
    "            all_count +=cur_sum\n",
    "        r = [hit/all_count for hit in hits_]\n",
    "        p_new = [hits_[i]/p_[i] for i in range(len(hits_))]\n",
    "        print 'Recall', r\n",
    "        print 'Precision', p_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9678.0, 16767.0, 22359.0, 26240.0, 29231.0, 31645.0, 33138.0, 34099.0, 34667.0, 35039.0]\n"
     ]
    }
   ],
   "source": [
    "# test warm up\n",
    "# relation_hits = []\n",
    "# for k in hits_k:\n",
    "#     relation_topk = tf.nn.top_k(pos_r_dec, k=k).indices\n",
    "#     relation_pred = tf.reduce_sum(tf.one_hot(relation_topk, tag_total), 1)\n",
    "#     relation_hits.append(tf.reduce_sum(tf.multiply(relation_pred, pos_r)))\n",
    "# vvvvv, bbbb = aminer.train.next_autoencoder_batch(10000, 50.0)\n",
    "# cur_hits = sess.run(relation_hits, feed_dict={pos_r: vvvvv, pos_br: bbbb})\n",
    "# print cur_hits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
